{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于SVM算法的意图识别\n",
    "在这个项目里， 你需要完成一个意图识别的任务，主要使用的模型是SVM算法。使用的数据集是`SMP2018中文人机对话技术评测`，是由科大讯飞股份有限公司提供数据。具体的数据可以从以下的链接下载： https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/#， 下载之后把是数据集解压在当前的工程的根目录下。 在这个任务里，你即将要完成的几个任务是：\n",
    "\n",
    "> 1. 读取文件、展示数据、并做并将label映射为数字index， 以及将index映射为类别。 \n",
    "> 2. 使用分词工具对用户query进行切分。\n",
    "> 3. 下载停用词表：https://github.com/goto456/stopwords/blob/master/%E4%B8%AD%E6%96%87%E5%81%9C%E7%94%A8%E8%AF%8D%E8%A1%A8.txt，   \n",
    "    读取， 并对分词后结果进行过滤\n",
    "> 4. 统计词频，并过滤低频词\n",
    "> 5. 计算`tfidf`\n",
    "> 6. 下载`tiny-word-embedding`：https://github.com/Embedding/Chinese-Word-Vectors, 分别使用`tfidf`, `word2vec` 建立`SVM`模型， 并对比二者结果。\n",
    "\n",
    "你需要完成的部分为标记为`TODO`的部分。 \n",
    "\n",
    "另外，提交作业时候的注意点：\n",
    "> 1. 不要试图去创建另外一个.ipynb文件，所有的程序需要在`starter_code.ipynb`里面实现。很多的模块已经帮你写好，不要试图去修改已经定义好的函数以及名字。 当然，自己可以按需求来创建新的函数。但一定要按照给定的框架来写程序，不然判作业的时候会出现很多问题。 \n",
    "> 2. 上传作业的时候把整个文件解压成.zip文件（不要.rar格式），请不要上传图片文件，其他的都需要上传包括`README.md`。\n",
    "> 3. 确保程序能够正常运行，我们支持的环境是`Python 3`,  千万不要使用`Python 2`\n",
    "> 4. 上传前一定要确保完整性，批改过一次的作业我们不会再重新批改，会作为最终的分数来对待。 \n",
    "> 5. 作业可以讨论，但请自己完成。让我们一起遵守贪心学院的`honor code`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba  # 感谢这个magic package，你不必要担心如何分词。如果想了解细节，可以参考此文件。\n",
    "import requests\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载数据，数据集为SMP2018\n",
    "#训练集数据保存在同目录下的train.json文件中，测试集数据保存在同目录下的test.json文件中\n",
    "#也可以使用！ wget https://worksheets.codalab.org/rest/bundles/0x0161fd2fb40d4dd48541c2643d04b0b8/contents/blob/ 方式下载\n",
    "if not os.path.exists('train.json'):\n",
    "    trainData = requests.get(\"https://worksheets.codalab.org/rest/bundles/0x0161fd2fb40d4dd48541c2643d04b0b8/contents/blob/\")\n",
    "    with open(\"train.json\", \"wb\") as f:\n",
    "         f.write(trainData.content)\n",
    "            \n",
    "if not os.path.exists('test.json'):\n",
    "    testData = requests.get(\"https://worksheets.codalab.org/rest/bundles/0x1f96bc12222641209ad057e762910252/contents/blob/\")\n",
    "    with open(\"test.json\", \"wb\") as f:\n",
    "         f.write(testData.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据至DataFrame中\n",
    "train_df = pd.read_json(\"train.json\").transpose()\n",
    "test_df = pd.read_json(\"test.json\").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据和测试数据: (2299, 2) (770, 2)\n",
      "标签的种类:  ['weather' 'map' 'cookbook' 'health' 'chat' 'train' 'calc' 'translation'\n",
      " 'music' 'tvchannel' 'poetry' 'telephone' 'stock' 'radio' 'contacts'\n",
      " 'lottery' 'website' 'video' 'news' 'bus' 'app' 'flight' 'epg' 'message'\n",
      " 'match' 'schedule' 'novel' 'riddle' 'email' 'datetime' 'cinemas']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天东莞天气如何</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>从观音桥到重庆市图书馆怎么走</td>\n",
       "      <td>map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鸭蛋怎么腌？</td>\n",
       "      <td>cookbook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>怎么治疗牛皮癣</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>唠什么</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query     label\n",
       "0        今天东莞天气如何   weather\n",
       "1  从观音桥到重庆市图书馆怎么走       map\n",
       "2          鸭蛋怎么腌？  cookbook\n",
       "3         怎么治疗牛皮癣    health\n",
       "4             唠什么      chat"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先来查看一下数据，确保没有任何错误！\n",
    "print (\"训练数据和测试数据:\", train_df.shape, test_df.shape)\n",
    "print (\"标签的种类: \", train_df.label.unique()) # 查看标签的个数以及标签种类，预计10个类别。\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName = train_df.label.unique() #全部label列表\n",
    "\n",
    "# TODO 实现文本label 与index的映射 hint : zip dict\n",
    "int_list = range(len(labelName))\n",
    "n_dict = dict(zip(labelName, int_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weather': 0, 'map': 1, 'cookbook': 2, 'health': 3, 'chat': 4, 'train': 5, 'calc': 6, 'translation': 7, 'music': 8, 'tvchannel': 9, 'poetry': 10, 'telephone': 11, 'stock': 12, 'radio': 13, 'contacts': 14, 'lottery': 15, 'website': 16, 'video': 17, 'news': 18, 'bus': 19, 'app': 20, 'flight': 21, 'epg': 22, 'message': 23, 'match': 24, 'schedule': 25, 'novel': 26, 'riddle': 27, 'email': 28, 'datetime': 29, 'cinemas': 30}\n"
     ]
    }
   ],
   "source": [
    "# TODO 查看label 与index 的映射关系\n",
    "print(n_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `TODO`: 统计一下每一个类别的文本各出现了几次？ 这有利于分析样本是否平衡还是不平衡。所谓的`不平衡样本`是有些类别的样本特别多，有些类别的样本特别少。这会引起模型训练的准确率。所以很重要一开始就要去看样本每个类别的个数。假如样本种类很多，即可以画出一个曲线出来。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chat           455\n",
       "cookbook       269\n",
       "video          182\n",
       "epg            107\n",
       "poetry         102\n",
       "tvchannel       71\n",
       "stock           71\n",
       "train           70\n",
       "map             68\n",
       "weather         66\n",
       "music           66\n",
       "message         63\n",
       "telephone       63\n",
       "flight          62\n",
       "translation     61\n",
       "news            58\n",
       "health          55\n",
       "website         54\n",
       "app             53\n",
       "riddle          34\n",
       "contacts        30\n",
       "schedule        29\n",
       "bus             24\n",
       "radio           24\n",
       "match           24\n",
       "novel           24\n",
       "calc            24\n",
       "cinemas         24\n",
       "lottery         24\n",
       "email           24\n",
       "datetime        18\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 统计并展示每一个类别出现的次数 hint groupby().count() / value_counts()\n",
    "train_df['label'].value_counts()\n",
    "#train_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 实现将index 数字与文本label的映射， 预测结果需要对比文本label\n",
    "train_df['labelIndex'] = train_df.label.map(n_dict)\n",
    "test_df['labelIndex'] = test_df.label.map(n_dict)\n",
    "\n",
    "# 查看 index 与 label 的映射关系\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>labelIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>毛泽东的诗哦。</td>\n",
       "      <td>poetry</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有房有车吗微笑</td>\n",
       "      <td>chat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013年亚洲冠军联赛恒广州恒大比赛时间。</td>\n",
       "      <td>match</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>若相惜不弃下一句是什么？</td>\n",
       "      <td>poetry</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>苹果翻译成英语</td>\n",
       "      <td>translation</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query        label  labelIndex\n",
       "0                毛泽东的诗哦。       poetry          10\n",
       "1                有房有车吗微笑         chat           4\n",
       "2  2013年亚洲冠军联赛恒广州恒大比赛时间。        match          24\n",
       "3           若相惜不弃下一句是什么？       poetry          10\n",
       "4                苹果翻译成英语  translation           7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 将dataframe 中文本label转换为数字。 hint:  map \n",
    "train_df[\"labelIndex\"] = train_df.label.map(n_dict)# TODO\n",
    "test_df[\"labelIndex\"] = test_df.label.map(n_dict)# TODO\n",
    "test_df.head()\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 对数据中的文本进行分词 hint:  jieba.cut\n",
    "\n",
    "    # jieba.cut 返回一个generator, 需要进行转换 hint: list\n",
    "\n",
    "def query_cut(query):\n",
    "    return [word for word in jieba.cut(query)]\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "train_df[\"queryCut\"] = train_df[\"query\"].apply(query_cut)\n",
    "test_df[\"queryCut\"] = test_df[\"query\"].apply(query_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>labelIndex</th>\n",
       "      <th>queryCut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天东莞天气如何</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "      <td>[今天, 东莞, 天气, 如何]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>从观音桥到重庆市图书馆怎么走</td>\n",
       "      <td>map</td>\n",
       "      <td>1</td>\n",
       "      <td>[从, 观音桥, 到, 重庆市, 图书馆, 怎么, 走]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鸭蛋怎么腌？</td>\n",
       "      <td>cookbook</td>\n",
       "      <td>2</td>\n",
       "      <td>[鸭蛋, 怎么, 腌, ？]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>怎么治疗牛皮癣</td>\n",
       "      <td>health</td>\n",
       "      <td>3</td>\n",
       "      <td>[怎么, 治疗, 牛皮癣]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>唠什么</td>\n",
       "      <td>chat</td>\n",
       "      <td>4</td>\n",
       "      <td>[唠, 什么]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query     label  labelIndex                      queryCut\n",
       "0        今天东莞天气如何   weather           0              [今天, 东莞, 天气, 如何]\n",
       "1  从观音桥到重庆市图书馆怎么走       map           1  [从, 观音桥, 到, 重庆市, 图书馆, 怎么, 走]\n",
       "2          鸭蛋怎么腌？  cookbook           2                [鸭蛋, 怎么, 腌, ？]\n",
       "3         怎么治疗牛皮癣    health           3                 [怎么, 治疗, 牛皮癣]\n",
       "4             唠什么      chat           4                       [唠, 什么]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看分词结果\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '?',\n",
       " '_',\n",
       " '“',\n",
       " '”',\n",
       " '、',\n",
       " '。',\n",
       " '《',\n",
       " '》',\n",
       " '一',\n",
       " '一些',\n",
       " '一何',\n",
       " '一切',\n",
       " '一则',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一来',\n",
       " '一样',\n",
       " '一般',\n",
       " '一转眼',\n",
       " '万一',\n",
       " '上',\n",
       " '上下',\n",
       " '下',\n",
       " '不',\n",
       " '不仅',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不只',\n",
       " '不外乎',\n",
       " '不如',\n",
       " '不妨',\n",
       " '不尽',\n",
       " '不尽然',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不料',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不至于',\n",
       " '不若']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 读取停用词\n",
    "with open(\"中文停用词表.txt\",\"r\", encoding='utf-8') as f:\n",
    "    stopWords = f.read().splitlines()\n",
    "\n",
    "# 查看停用词\n",
    "stopWords[0:59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 使用停止词过滤上一步分词结果\n",
    "\n",
    "def rm_stop_word(wordList):\n",
    "    result = []\n",
    "    for word in wordList:\n",
    "        if word not in stopWords:\n",
    "            result.append(word)\n",
    "    return result\n",
    "    # TODO\n",
    "\n",
    "train_df[\"queryCutRMStopWord\"] = train_df[\"queryCut\"].apply(rm_stop_word)\n",
    "test_df[\"queryCutRMStopWord\"] = test_df[\"queryCut\"].apply(rm_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>labelIndex</th>\n",
       "      <th>queryCut</th>\n",
       "      <th>queryCutRMStopWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天东莞天气如何</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "      <td>[今天, 东莞, 天气, 如何]</td>\n",
       "      <td>[今天, 东莞, 天气]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>从观音桥到重庆市图书馆怎么走</td>\n",
       "      <td>map</td>\n",
       "      <td>1</td>\n",
       "      <td>[从, 观音桥, 到, 重庆市, 图书馆, 怎么, 走]</td>\n",
       "      <td>[观音桥, 重庆市, 图书馆, 走]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鸭蛋怎么腌？</td>\n",
       "      <td>cookbook</td>\n",
       "      <td>2</td>\n",
       "      <td>[鸭蛋, 怎么, 腌, ？]</td>\n",
       "      <td>[鸭蛋, 腌]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>怎么治疗牛皮癣</td>\n",
       "      <td>health</td>\n",
       "      <td>3</td>\n",
       "      <td>[怎么, 治疗, 牛皮癣]</td>\n",
       "      <td>[治疗, 牛皮癣]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>唠什么</td>\n",
       "      <td>chat</td>\n",
       "      <td>4</td>\n",
       "      <td>[唠, 什么]</td>\n",
       "      <td>[唠]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query     label  labelIndex                      queryCut  \\\n",
       "0        今天东莞天气如何   weather           0              [今天, 东莞, 天气, 如何]   \n",
       "1  从观音桥到重庆市图书馆怎么走       map           1  [从, 观音桥, 到, 重庆市, 图书馆, 怎么, 走]   \n",
       "2          鸭蛋怎么腌？  cookbook           2                [鸭蛋, 怎么, 腌, ？]   \n",
       "3         怎么治疗牛皮癣    health           3                 [怎么, 治疗, 牛皮癣]   \n",
       "4             唠什么      chat           4                       [唠, 什么]   \n",
       "\n",
       "   queryCutRMStopWord  \n",
       "0        [今天, 东莞, 天气]  \n",
       "1  [观音桥, 重庆市, 图书馆, 走]  \n",
       "2             [鸭蛋, 腌]  \n",
       "3           [治疗, 牛皮癣]  \n",
       "4                 [唠]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看过滤停止词后的结果\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO 计算词频 hint collections.Counter()\n",
    "allWords = [word for query in train_df.queryCutRMStopWord for word in query] #所有词组成的列表\n",
    "freWord = Counter(allWords)#统计词频，一个字典，键为词，值为词出现的次数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 过滤低频词\n",
    "highFreWords = [word for word in freWord.keys() if freWord[word]>3] #词频超过3的词列表\n",
    "def rm_low_fre_word(query):\n",
    "    result = []\n",
    "    for word in query:\n",
    "        if word in highFreWords:\n",
    "            result.append(word)\n",
    "    return result\n",
    "    # TODO\n",
    "    \n",
    "#去除低频词\n",
    "train_df[\"queryFinal\"] = train_df[\"queryCutRMStopWord\"].apply(rm_low_fre_word)\n",
    "test_df[\"queryFinal\"] = test_df[\"queryCutRMStopWord\"].apply(rm_low_fre_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 计算`TFIDF`。\n",
    "这部分是将文本数据转化为计算机可以识别的类型。 tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "$$w_{i,j} = tf_{i, j} * log(\\frac{N}{df_{i}})$$\n",
    "\n",
    "\n",
    "主要使用sklearn 实现， 详细文档参考： https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "`tfidf`有几个关键参数  \n",
    "> ngram_range: tuple(min_n, max_n) 要提取的n-gram的n-values的下限和上限范围，在min_n <= n <= max_n区间的n的全部值  \n",
    "> stop_words：string {'english'}, list, or None(default)   \n",
    "    如果为english，用于英语内建的停用词列表  \n",
    "    如果为list，该列表被假定为包含停用词，列表中的所有词都将从令牌中删除  \n",
    "    如果None，不使用停用词。max_df可以被设置为范围[0.7, 1.0)的值，基于内部预料词频来自动检测和过滤停用词  \n",
    "> max_df： float in range [0.0, 1.0] or int, optional, 1.0 by default    \n",
    "    当构建词汇表时，严格忽略高于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。  \n",
    "> min_df：float in range [0.0, 1.0] or int, optional, 1.0 by default    \n",
    "    当构建词汇表时，严格忽略低于给出阈值的文档频率的词条，语料指定的停用词。如果是浮点值，该参数代表文档的比例，整型绝对计数值，如果词汇表不为None，此参数被忽略。  \n",
    "> max_features： optional， None by default    \n",
    "    如果不为None，构建一个词汇表，仅考虑max_features--按语料词频排序，如果词汇表不为None，这个参数被忽略  \n",
    "> norm：'l1', 'l2', or None,optional  \n",
    "    范数用于标准化词条向量。None为不归一化  \n",
    "> smooth_idf：boolean，optional  \n",
    "    通过加1到文档频率平滑idf权重，为防止除零，加入一个额外的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 将分词且过滤后的文本数据转化为tfidf 形式： \n",
    "trainText = [' '.join(query) for query in train_df[\"queryFinal\"]]\n",
    "testText = [' '.join(query) for query in test_df[\"queryFinal\"]]\n",
    "allText = trainText+testText\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf = tfidf.fit_transform(allText)\n",
    "\n",
    "# sklearn tfidf vector fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 切分数据集 hint sklearn train_test_split()\n",
    "trainLen = len(train_df)\n",
    "train_x_tfidf = tfidf.toarray()[0:trainLen]\n",
    "test_x_tfidf = tfidf.toarray()[trainLen:]\n",
    "train_y_tfidf = train_df[\"labelIndex\"]\n",
    "test_y_tfidf = test_df[\"labelIndex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_tfidf.shape = (2299, 238)\n",
      "train_y_tfidf.shape = (2299,)\n",
      "test_x_tfidf.shape = (770, 238)\n",
      "test_y_tfidf.shape = (770,)\n"
     ]
    }
   ],
   "source": [
    "# 切分后的数据信息\n",
    "print(\"train_x_tfidf.shape =\",train_x_tfidf.shape)\n",
    "print(\"train_y_tfidf.shape =\",train_y_tfidf.shape)\n",
    "print(\"test_x_tfidf.shape =\",test_x_tfidf.shape)\n",
    "print(\"test_y_tfidf.shape =\",test_y_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取embedding\n",
    "with open(\"tiny_word2vec.pickle\",\"rb\") as f:\n",
    "    word2vec = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#词向量举例\n",
    "word2vec[\"今天\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 加载`word embedding`。\n",
    "词嵌入是自然语言处理中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。\n",
    "\n",
    "有很多pretrained embedding， 有很多加载方式， `gensim` 提供了很多有用的功能， 详细文档参考： https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "> 1.你需要做的是使用加载好的pretrained embedding， 将过滤后的分词转换为相应的词向量  \n",
    "> 2.可能存在一些单词不在pretrained 的单词中， 这时候可以使用numpy 随机生成相同纬度的向量  \n",
    "> 3.一个句子会有多个单词， 对于模型input会不一致， 一种简单的方法是对一个句子的所有词向量求平均值。 最后结果应该是（1， embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入预训练好的词向量，词向量来源：https://github.com/Embedding/Chinese-Word-Vectors\n",
    "with open(\"tiny_word2vec.pickle\",\"rb\") as f:\n",
    "    word2vec = pickle.load(f)\n",
    "# 词向量举例\n",
    "word2vec[\"今天\"]\n",
    "\n",
    "# TODO 将过滤后的分词文本转换为相同维度的向量\n",
    "vocabulary = word2vec.keys()\n",
    "def sentence2vec(query):\n",
    "    result = []\n",
    "    for word in query:\n",
    "        if word in word2vec:\n",
    "            result.append(word2vec[word])\n",
    "    result = np.array(result)\n",
    "    if len(result)>0:\n",
    "        result = result.sum(axis=0)/len(result)\n",
    "    else:\n",
    "        result = np.zeros(shape=(300,))\n",
    "    return result\n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将转换为词向量的数据， 切分为训练集， 验证集\n",
    "train_x_vec = np.vstack(train_df[\"queryCutRMStopWord\"].apply(sentence2vec))\n",
    "test_x_vec = np.vstack(test_df[\"queryCutRMStopWord\"].apply(sentence2vec))\n",
    "train_y_vec = train_df[\"labelIndex\"]\n",
    "test_y_vec = test_df[\"labelIndex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_vec.shape = (2299, 300)\n",
      "train_y_vec.shape = (2299,)\n",
      "test_x_vec.shape = (770, 300)\n",
      "test_y_vec.shape = (770,)\n"
     ]
    }
   ],
   "source": [
    "# 查看切分后的数据信息\n",
    "print(\"train_x_vec.shape =\",train_x_vec.shape)\n",
    "print(\"train_y_vec.shape =\",train_y_vec.shape)\n",
    "print(\"test_x_vec.shape =\",test_x_vec.shape)\n",
    "print(\"test_y_vec.shape =\",test_y_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `支持向量机`（英语：`support vector machine`，常简称为`SVM`）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。除了进行线性分类之外，SVM还可以使用所谓的核技巧有效地进行非线性分类，将其输入隐式映射到高维特征空间中。\n",
    "\n",
    "我们在此主要使用sklearn来建立`SVM`模型（感谢sklearn), 更多相关文档参考：https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html    \n",
    "`SVM` 关键参数:  \n",
    "> C：C-SVC的惩罚参数C?默认值是1.0  \n",
    "C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。\n",
    "> kernel ：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’   \n",
    "  　　0 – 线性：u'v  \n",
    " 　　 1 – 多项式：(gamma*u'*v + coef0)^degree  \n",
    "  　　2 – RBF函数：exp(-gamma|u-v|^2)  \n",
    "  　　3 –sigmoid：tanh(gamma*u'*v + coef0)  \n",
    "> degree ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。  \n",
    "> gamma ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features  \n",
    "> coef0 ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。  \n",
    "> probability ：是否采用概率估计？.默认为False  \n",
    "> shrinking ：是否采用shrinking heuristic方法，默认为true  \n",
    "> decision_function_shape ：‘ovo’, ‘ovr’ or None, default=None3 \n",
    "\n",
    "接下来你需要做的是：\n",
    "> 1. 分别使用tfidf, embedding建立线性SVM，以及非线性SVM\n",
    "> 2. 对比模型结果，选择最优模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  4  4 ... 22  4 12]\n",
      "train accuracy 0.727707698999565\n",
      "train F1_score 0.7708505943829783\n",
      "test accuracy 0.6831168831168831\n",
      "test F1_score 0.6954557599168526\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用tfidf 特征建立线性SVM模型 hint: SVC()\n",
    "from sklearn.svm import SVC\n",
    "tfidfLinearSVM= SVC(C=1, kernel='linear', decision_function_shape='ovr') \n",
    "tfidfLinearSVM.fit(train_x_tfidf, train_y_tfidf)\n",
    "print(tfidfLinearSVM.predict(train_x_tfidf))\n",
    "# 输出模型结果， accuracy,  F1_score\n",
    "\n",
    "print('train accuracy %s' % metrics.accuracy_score(train_y_tfidf, tfidfLinearSVM.predict(train_x_tfidf)))\n",
    "print('train F1_score %s' % metrics.f1_score(train_y_tfidf, tfidfLinearSVM.predict(train_x_tfidf),average=\"macro\"))\n",
    "print('test accuracy %s' % metrics.accuracy_score(test_y_tfidf, tfidfLinearSVM.predict(test_x_tfidf)))\n",
    "print('test F1_score %s' % metrics.f1_score(test_y_tfidf, tfidfLinearSVM.predict(test_x_tfidf),average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.745541539799913\n",
      "train F1_score 0.7984100288786787\n",
      "test accuracy 0.6883116883116883\n",
      "test F1_score 0.7020022181879286\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用tfidf 特征建立`rbf` SVM 模型\n",
    "tfidfKernelizedSVM = SVC(C=1, kernel='rbf', decision_function_shape='ovr')\n",
    "tfidfKernelizedSVM.fit(train_x_tfidf, train_y_tfidf)\n",
    "\n",
    "# 输出模型结果， accuracy,  F1_score\n",
    "\n",
    "print('train accuracy %s' % metrics.accuracy_score(train_y_tfidf, tfidfKernelizedSVM.predict(train_x_tfidf)))\n",
    "print('train F1_score %s' % metrics.f1_score(train_y_tfidf, tfidfKernelizedSVM.predict(train_x_tfidf),average=\"macro\"))\n",
    "print('test accuracy %s' % metrics.accuracy_score(test_y_tfidf, tfidfKernelizedSVM.predict(test_x_tfidf)))\n",
    "print('test F1_score %s' % metrics.f1_score(test_y_tfidf, tfidfKernelizedSVM.predict(test_x_tfidf),average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9778164419312745\n",
      "train F1_score 0.9818051685775911\n",
      "test accuracy 0.8467532467532467\n",
      "test F1_score 0.8565368406833987\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用embeding 特征建立线性SVM模型\n",
    "word2vecLinearSVM= SVC(C=1, kernel='linear', decision_function_shape='ovr') \n",
    "word2vecLinearSVM.fit(train_x_vec, train_y_vec)\n",
    "\n",
    "# 输出模型结果， accuracy,  F1_score\n",
    "\n",
    "print('train accuracy %s' % metrics.accuracy_score(train_y_vec, word2vecLinearSVM.predict(train_x_vec)))\n",
    "print('train F1_score %s' % metrics.f1_score(train_y_vec, word2vecLinearSVM.predict(train_x_vec),average=\"macro\"))\n",
    "print('test accuracy %s' % metrics.accuracy_score(test_y_vec, word2vecLinearSVM.predict(test_x_vec)))\n",
    "print('test F1_score %s' % metrics.f1_score(test_y_vec, word2vecLinearSVM.predict(test_x_vec),average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9386689865158765\n",
      "train F1_score 0.9353701170287393\n",
      "test accuracy 0.8428571428571429\n",
      "test F1_score 0.840815869158248\n"
     ]
    }
   ],
   "source": [
    "# TODO 使用embedding  特征建立`rbf` SVM模型\n",
    "word2vecKernelizedSVM= SVC(C=1, kernel='rbf', decision_function_shape='ovr') \n",
    "word2vecKernelizedSVM.fit(train_x_vec, train_y_vec)\n",
    "\n",
    "# 输出模型结果， accuracy,  F1_score\n",
    "\n",
    "print('train accuracy %s' % metrics.accuracy_score(train_y_vec, word2vecKernelizedSVM.predict(train_x_vec)))\n",
    "print('train F1_score %s' % metrics.f1_score(train_y_vec, word2vecKernelizedSVM.predict(train_x_vec),average=\"macro\"))\n",
    "print('test accuracy %s' % metrics.accuracy_score(test_y_vec, word2vecKernelizedSVM.predict(test_x_vec)))\n",
    "print('test F1_score %s' % metrics.f1_score(test_y_vec, word2vecKernelizedSVM.predict(test_x_vec),average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
